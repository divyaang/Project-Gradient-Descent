{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_gradient(points, learning_rate, m):\n",
    "    m_slope = np.zeros(points.shape[1])\n",
    "    \n",
    "    M = len(points)\n",
    "    for i in range(M):\n",
    "        x = points[i,:(points.shape[1]-1)]\n",
    "        x = np.append(x,1)\n",
    "        y = points[i,points.shape[1]-1]\n",
    "        \n",
    "        for j in range(points.shape[1]):\n",
    "            m_slope[j] += (-2/M)* (y-(m*x).sum())*x[j]\n",
    "    \n",
    "    new_m = m - learning_rate*m_slope\n",
    "    \n",
    "    return new_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd(points, learning_rate, num_iterations):\n",
    "    m = np.zeros(points.shape[1])\n",
    "    for i in range(num_iterations):\n",
    "        m = step_gradient(points, learning_rate, m)\n",
    "        print((i+1), \" Cost: \", cost(points, m))\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(points, m):\n",
    "    total_cost = 0\n",
    "    M = len(points)\n",
    "    for i in range(M):\n",
    "        x = points[i][:(points.shape[1]-1)]\n",
    "        x = np.append(x,1)\n",
    "        y = points[i][points.shape[1]-1]\n",
    "        total_cost += (1/M)*(((y-(m*x)).sum())**2)\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(data, m):\n",
    "    ms = m[:-1]\n",
    "    c = m[-1]\n",
    "    ans = []\n",
    "    \n",
    "    for d in data:\n",
    "        ans.append((ms*d).sum() + c)\n",
    "        \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(data, learning_rate, num_iterations):\n",
    "    #learning_rate = 0.15\n",
    "    #num_iterations = 100\n",
    "    m = gd(data, learning_rate, num_iterations)\n",
    "    print(m)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.loadtxt('train_power_plant.csv', delimiter=',')\n",
    "test_data = np.loadtxt('test_power_plant.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  Cost:  4374836.5812505875\n",
      "2  Cost:  3930688.5615794524\n",
      "3  Cost:  3675638.0678989305\n",
      "4  Cost:  3526714.910480043\n",
      "5  Cost:  3438837.3733827863\n",
      "6  Cost:  3386640.6074158354\n",
      "7  Cost:  3355511.774732079\n",
      "8  Cost:  3336901.3068856043\n",
      "9  Cost:  3325757.953714549\n",
      "10  Cost:  3319079.1925762803\n",
      "11  Cost:  3315073.6487587383\n",
      "12  Cost:  3312670.1292775935\n",
      "13  Cost:  3311227.211432695\n",
      "14  Cost:  3310360.495688122\n",
      "15  Cost:  3309839.496664083\n",
      "16  Cost:  3309525.971367481\n",
      "17  Cost:  3309336.985450745\n",
      "18  Cost:  3309222.7776210634\n",
      "19  Cost:  3309153.486925878\n",
      "20  Cost:  3309111.192051281\n",
      "21  Cost:  3309085.1358113163\n",
      "22  Cost:  3309068.86005695\n",
      "23  Cost:  3309058.486611273\n",
      "24  Cost:  3309051.6857574983\n",
      "25  Cost:  3309047.057251631\n",
      "26  Cost:  3309043.758858043\n",
      "27  Cost:  3309041.2834114847\n",
      "28  Cost:  3309039.3250060566\n",
      "29  Cost:  3309037.6986701363\n",
      "30  Cost:  3309036.2921407525\n",
      "31  Cost:  3309035.0369036635\n",
      "32  Cost:  3309033.890797968\n",
      "33  Cost:  3309032.827563086\n",
      "34  Cost:  3309031.8305545\n",
      "35  Cost:  3309030.8889628067\n",
      "36  Cost:  3309029.995537088\n",
      "37  Cost:  3309029.1452117846\n",
      "38  Cost:  3309028.3342777076\n",
      "39  Cost:  3309027.5598794357\n",
      "40  Cost:  3309026.819710292\n",
      "41  Cost:  3309026.111825866\n",
      "42  Cost:  3309025.434529297\n",
      "43  Cost:  3309024.786300138\n",
      "44  Cost:  3309024.1657494325\n",
      "45  Cost:  3309023.5715911947\n",
      "46  Cost:  3309023.0026233103\n",
      "47  Cost:  3309022.457715021\n",
      "48  Cost:  3309021.935797892\n",
      "49  Cost:  3309021.4358592853\n",
      "50  Cost:  3309020.9569374006\n",
      "51  Cost:  3309020.4981173873\n",
      "52  Cost:  3309020.0585279977\n",
      "53  Cost:  3309019.6373388483\n",
      "54  Cost:  3309019.2337580244\n",
      "55  Cost:  3309018.847029892\n",
      "56  Cost:  3309018.4764330676\n",
      "57  Cost:  3309018.121278778\n",
      "58  Cost:  3309017.7809090842\n",
      "59  Cost:  3309017.454695316\n",
      "60  Cost:  3309017.142036742\n",
      "61  Cost:  3309016.842359137\n",
      "62  Cost:  3309016.5551135098\n",
      "63  Cost:  3309016.279774944\n",
      "64  Cost:  3309016.015841442\n",
      "65  Cost:  3309015.762832879\n",
      "66  Cost:  3309015.520289945\n",
      "67  Cost:  3309015.287773227\n",
      "68  Cost:  3309015.064862317\n",
      "69  Cost:  3309014.851154844\n",
      "70  Cost:  3309014.6462658327\n",
      "71  Cost:  3309014.4498267137\n",
      "72  Cost:  3309014.2614847664\n",
      "73  Cost:  3309014.0809022863\n",
      "74  Cost:  3309013.9077559956\n",
      "75  Cost:  3309013.7417363785\n",
      "76  Cost:  3309013.5825470197\n",
      "77  Cost:  3309013.4299041294\n",
      "78  Cost:  3309013.28353589\n",
      "79  Cost:  3309013.143181998\n",
      "80  Cost:  3309013.0085931444\n",
      "81  Cost:  3309012.879530492\n",
      "82  Cost:  3309012.755765284\n",
      "83  Cost:  3309012.637078396\n",
      "84  Cost:  3309012.523259865\n",
      "85  Cost:  3309012.4141085786\n",
      "86  Cost:  3309012.309431798\n",
      "87  Cost:  3309012.2090448714\n",
      "88  Cost:  3309012.1127709034\n",
      "89  Cost:  3309012.0204403084\n",
      "90  Cost:  3309011.931890582\n",
      "91  Cost:  3309011.846966061\n",
      "92  Cost:  3309011.7655175286\n",
      "93  Cost:  3309011.687401959\n",
      "94  Cost:  3309011.6124822847\n",
      "95  Cost:  3309011.540627096\n",
      "96  Cost:  3309011.4717104523\n",
      "97  Cost:  3309011.4056117036\n",
      "98  Cost:  3309011.3422151064\n",
      "99  Cost:  3309011.2814097484\n",
      "100  Cost:  3309011.223089286\n",
      "101  Cost:  3309011.1671517943\n",
      "102  Cost:  3309011.113499553\n",
      "103  Cost:  3309011.062038896\n",
      "104  Cost:  3309011.0126799727\n",
      "105  Cost:  3309010.965336657\n",
      "106  Cost:  3309010.9199264375\n",
      "107  Cost:  3309010.8763701217\n",
      "108  Cost:  3309010.8345917906\n",
      "109  Cost:  3309010.794518706\n",
      "110  Cost:  3309010.756081074\n",
      "111  Cost:  3309010.719211956\n",
      "112  Cost:  3309010.683847192\n",
      "113  Cost:  3309010.649925296\n",
      "114  Cost:  3309010.6173872603\n",
      "115  Cost:  3309010.586176497\n",
      "116  Cost:  3309010.5562387435\n",
      "117  Cost:  3309010.52752197\n",
      "118  Cost:  3309010.4999763253\n",
      "119  Cost:  3309010.473553945\n",
      "120  Cost:  3309010.4482089356\n",
      "121  Cost:  3309010.4238973022\n",
      "122  Cost:  3309010.40057683\n",
      "123  Cost:  3309010.378207068\n",
      "124  Cost:  3309010.3567492138\n",
      "125  Cost:  3309010.33616601\n",
      "126  Cost:  3309010.316421731\n",
      "127  Cost:  3309010.2974822293\n",
      "128  Cost:  3309010.2793145743\n",
      "129  Cost:  3309010.261887307\n",
      "130  Cost:  3309010.2451702263\n",
      "131  Cost:  3309010.2291343734\n",
      "132  Cost:  3309010.21375191\n",
      "133  Cost:  3309010.1989962445\n",
      "134  Cost:  3309010.1848417623\n",
      "135  Cost:  3309010.1712639676\n",
      "136  Cost:  3309010.158239347\n",
      "137  Cost:  3309010.145745335\n",
      "138  Cost:  3309010.1337603056\n",
      "139  Cost:  3309010.1222635107\n",
      "140  Cost:  3309010.111235033\n",
      "141  Cost:  3309010.1006557895\n",
      "142  Cost:  3309010.090507464\n",
      "143  Cost:  3309010.0807725084\n",
      "144  Cost:  3309010.071434049\n",
      "145  Cost:  3309010.062475959\n",
      "146  Cost:  3309010.0538826934\n",
      "147  Cost:  3309010.0456394306\n",
      "148  Cost:  3309010.0377318505\n",
      "149  Cost:  3309010.03014635\n",
      "150  Cost:  3309010.0228697388\n",
      "151  Cost:  3309010.015889477\n",
      "152  Cost:  3309010.0091934744\n",
      "153  Cost:  3309010.0027701347\n",
      "154  Cost:  3309009.996608392\n",
      "155  Cost:  3309009.990697527\n",
      "156  Cost:  3309009.985027374\n",
      "157  Cost:  3309009.9795880974\n",
      "158  Cost:  3309009.974370308\n",
      "159  Cost:  3309009.9693649793\n",
      "160  Cost:  3309009.964563449\n",
      "161  Cost:  3309009.9599574264\n",
      "162  Cost:  3309009.9555389485\n",
      "163  Cost:  3309009.951300364\n",
      "164  Cost:  3309009.947234371\n",
      "165  Cost:  3309009.943333922\n",
      "166  Cost:  3309009.9395922655\n",
      "167  Cost:  3309009.93600299\n",
      "168  Cost:  3309009.932559808\n",
      "169  Cost:  3309009.9292568075\n",
      "170  Cost:  3309009.926088281\n",
      "171  Cost:  3309009.923048776\n",
      "172  Cost:  3309009.920133028\n",
      "173  Cost:  3309009.9173359587\n",
      "174  Cost:  3309009.9146527783\n",
      "175  Cost:  3309009.9120788155\n",
      "176  Cost:  3309009.9096096675\n",
      "177  Cost:  3309009.907241024\n",
      "178  Cost:  3309009.9049688196\n",
      "179  Cost:  3309009.902789103\n",
      "180  Cost:  3309009.900698131\n",
      "181  Cost:  3309009.8986922763\n",
      "182  Cost:  3309009.8967681\n",
      "183  Cost:  3309009.894922254\n",
      "184  Cost:  3309009.893151525\n",
      "185  Cost:  3309009.8914528987\n",
      "186  Cost:  3309009.88982341\n",
      "187  Cost:  3309009.888260253\n",
      "188  Cost:  3309009.8867607573\n",
      "189  Cost:  3309009.885322294\n",
      "190  Cost:  3309009.8839423694\n",
      "191  Cost:  3309009.8826186405\n",
      "192  Cost:  3309009.881348788\n",
      "193  Cost:  3309009.8801306305\n",
      "194  Cost:  3309009.8789620576\n",
      "195  Cost:  3309009.877841044\n",
      "196  Cost:  3309009.8767656847\n",
      "197  Cost:  3309009.87573407\n",
      "198  Cost:  3309009.874744492\n",
      "199  Cost:  3309009.8737951834\n",
      "200  Cost:  3309009.8728844975\n",
      "[-1.49007707e+01 -2.89572783e+00  3.50104499e-01 -2.34166311e+00\n",
      "  4.54431293e+02]\n"
     ]
    }
   ],
   "source": [
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(train_data[:,:-1])\n",
    "\n",
    "fs = scaler.transform(train_data[:,:-1])\n",
    "\n",
    "train = np.insert(fs, train_data.shape[1]-1, train_data[:,-1], axis=1)\n",
    "test = scaler.transform(test_data)\n",
    "\n",
    "m1 = run(train, 0.2, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans1  = pred(test, m1)\n",
    "np.savetxt(\"predictions_power_plant.csv\", ans1, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(y_truth, y_pred):\n",
    "    u = ((y_truth - y_pred)**2).sum()\n",
    "    v = ((y_truth - y_truth.mean())**2).sum()\n",
    "    return 1 - u/v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of determination based on Training Data (With Feature Scaling) is: 0.9287631977982681\n"
     ]
    }
   ],
   "source": [
    "s1 = pred(train[:,:-1], m1)\n",
    "print('Coefficient of determination based on Training Data (With Feature Scaling) is:',score(train[:,-1], s1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
